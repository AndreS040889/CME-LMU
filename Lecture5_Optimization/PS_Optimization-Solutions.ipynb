{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Problem Set 3 - Numerical Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2018-03-02 15:11:04.014408\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.linalg\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (N)\n",
    "\n",
    "*From Judd(1998), chapter 4, question 2*. One of the classical uses of optimization is the computation of the *Pareto frontier*. Consider the endowment economy with $m$ goods and $n$ agents. Assume that agent $i$'s utility function over the $m$ goods is \n",
    "\n",
    "\\begin{equation}\n",
    "    u^{i}(x^i) = \\sum^{m}_{j = 1} a^i_j (x^i_j)^{v^i_j + 1} (1 + v^i_j)^{-1} \n",
    "\\end{equation}\n",
    "\n",
    "Suppose that agent $i$'s endowment of good $j$ is $e^i_j$. Assume that $a^i_j, e^i_j > 0 > v^i_j$ (for $v^i_j =-1$, we replace $(x^i_j)^{v^i_j + 1} (1 + v^i_j)^{-1}$ with $\\ln x^i_j$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Write a program using Scipy's BFGS implementation that will read in the $v^i_j$, $a^i_j$ and $e^i_j$ and the social weights $\\lambda^i$, and output the solution to the social planner's problem. Choose $m = n = 2$ and solve the problem *analytically* for $\\lambda_1 = \\lambda_2 = 0.5$ and the following values for the remaining parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = [[-4 -2]\n",
      " [-3 -3]]\n",
      "A = [[1 4]\n",
      " [1 8]]\n",
      "E = [[6 4]\n",
      " [5 1]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[6, 4], [5, 1]])\n",
    "V = np.array([[-4, -2], [-3, -3]])\n",
    "A = np.array([[1, 4], [1, 8]])\n",
    "\n",
    "print(\"V = {}\".format(V) )\n",
    "print(\"A = {}\".format(A) )\n",
    "print(\"E = {}\".format(E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to read these matrices is that a good corresponds to a row and an agent to a column. For example, agent 1's endowment of good 2, $e^1_2$, would be the element in the second row and first column of matrix **E**, and hence $e^1_2 = 5$.\n",
    "\n",
    "With these parameter values, confirm your analytical result equals the numerical output of your program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Test your program for higher numbers of goods and agents. You can create the parameter matrices above using Numpy's **np.random.uniform** function. Can your program handle $m = n = 5$? $m = n = 10$?      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: A slightly tricky issue when answering this question using *unconstrained* numerical optimization methods is how to deal with the constraint that aggregate consumption of good $j$ must equal aggregate endowments, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum^{n}_{i = 1} x_j^i = \\sum^{n}_{i = 1} e_j^i\n",
    "\\end{equation}\n",
    "\n",
    "One way to address this is to have the algorithm solve for the optimal consumption for $n - 1$ agents and evaluate the consumption and hence the utility of the last agent *as the residual*. formally, for good $j$,\n",
    "\n",
    "\\begin{equation}\n",
    "    x_j^n = \\sum^{n}_{i = 1} e_j^i - \\sum^{n-1}_{i = 1} x_j^i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "For the analytical solution to the 2-by-2 case, compare the notes in class. \n",
    "\n",
    "For the numerical implementation, start by addressing the issue of the endowment constraints discussed above. The function **consum(x, E)** takes a vector **x** (the consumption levels for the first $n-1$ agents) and the endowment matrix **E** and returns a consumption matrix **X**, in which the last column (the consumption of agent $n$) is computed as the residual.\n",
    "\n",
    "As a side note, why do we define **consum** in a way that it takes a vector (i.e. a one-dimensional array) rather than a two-dimensional matrix as an input? Note that **scipy.optimize.minimize** returns a vector (even if we use a matrix as an initial, as seen below!) and hence we need to write **consum** in a way that it accommodates this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consum(x, E):\n",
    "    \"\"\"\n",
    "    Computes an m-by-n consumption matrix from solution vector x and endowments E;\n",
    "    Agent n's consumption is computed as the residual comsumption \n",
    "    \"\"\"\n",
    "    ## get dimension of the endowment matrix\n",
    "    m, n = E.shape \n",
    "    ## check that x and E have consistent dimensions\n",
    "    assert len(x) == m * (n-1), \"The length is not consistent with the dimensions of E!\"\n",
    "    \n",
    "    ## reshape x into matrix X \n",
    "    X = x.copy()\n",
    "    X.shape = m, n - 1\n",
    "    \n",
    "    ## compute residual consumption\n",
    "    x_res = E.sum(axis = 1) - X.sum(axis = 1)\n",
    "    \n",
    "    ## combine X with x_res and return\n",
    "    return np.column_stack( (X, x_res) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **consum**, it is straightforward to implement the objective function $U = \\sum^n_{i = 1} \\lambda^i u^{i}(x^i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, V, A, E, lam):\n",
    "    \"\"\"\n",
    "    Computes the objective function for the m-by-n social planner problem\n",
    "    \"\"\"\n",
    "    ## get dimension of the problem \n",
    "    m, n = V.shape ## number of agents, number of good\n",
    "\n",
    "    ## check if the parameter matrices have the correct dimensions \n",
    "    assert A.shape == (m, n), \"The dimensions of A and V must coincide!\"\n",
    "    assert E.shape == (m, n), \"The dimensions of E and V must coincide!\"\n",
    "    assert len(lam) == n, \"The length of lam is not consistent with the dimensions of V!\"\n",
    "    \n",
    "    ## compute matrices of consumption using x and endowment matrix E\n",
    "    X = consum(x, E)\n",
    "    \n",
    "    ## loop over agents and evaluate their utility\n",
    "    u = np.zeros(n)\n",
    "    for ind in range(n):\n",
    "        v, a = V[:, ind], A[:, ind]\n",
    "        Q = a / (1 + v)\n",
    "        u[ind] = Q @ X[:,ind]**(1 + v)\n",
    "\n",
    "    ## compute welfare given weights lam (NB: we could use summation instead of vector multiplication )\n",
    "    return - lam @ u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the problem using the BFGS algorithm for 2-by-2 case. As an initial guess, use the agents' endowment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.458333\n",
      "         Iterations: 16\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 18\n",
      "Consumption matrix: \n",
      "[[ 1.99999254  8.00000746]\n",
      " [ 2.00000424  3.99999576]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[6, 4], [5, 1]])\n",
    "V = np.array([[-4, -2], [-3, -3]])\n",
    "A = np.array([[1, 4], [1, 8]])\n",
    "\n",
    "x0 =  E[:, :-1]\n",
    "lam = np.array([0.5, 0.5])\n",
    "objective(x0, V, A, E, lam)\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m = 3, n = 2\n",
    "\n",
    "We can check that the algorithm works in a setting with more goods than agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.329515\n",
      "         Iterations: 8\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 9\n",
      "Consumption matrix: \n",
      "[[ 1.69723441  1.30276559]\n",
      " [ 1.47265462  2.52734538]\n",
      " [ 1.34772323  3.65227677]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[2, 1], [2, 2], [2, 3]])\n",
    "V = np.array([[-2, -4], [-3, -2], [-5, -2]])\n",
    "A = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "\n",
    "x0 =  E[:, :-1]\n",
    "lam = np.array([0.5, 0.5])\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m = 6, n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 6, 6\n",
    "E = np.random.uniform(1, 5, (m, n))\n",
    "V = np.random.uniform(-5, -1.001, (m, n))\n",
    "A = np.random.uniform(1, 3, (m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 10.967842\n",
      "         Iterations: 102\n",
      "         Function evaluations: 3360\n",
      "         Gradient evaluations: 105\n",
      "Consumption matrix: \n",
      "[[ 1.82542053  2.62002612  1.70209792  5.29374389  1.52269862  1.43355682]\n",
      " [ 2.01626758  4.19836841  1.65491694  3.02481164  5.14244895  1.43555422]\n",
      " [ 2.24497596  4.78879531  3.25923787  2.21562732  4.03663922  2.6169814 ]\n",
      " [ 2.65812316  2.78397299  7.67766336  2.20250723  2.07490595  3.98597358]\n",
      " [ 1.97880128  2.45690871  7.03076927  2.10501375  2.33664977  2.52748484]\n",
      " [ 1.80864154  1.49248406  8.96068465  1.76144698  1.65472094  2.88555589]]\n"
     ]
    }
   ],
   "source": [
    "x0 = E[:, :-1]\n",
    "lam = np.ones(n) * (1/n)\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (N)\n",
    "\n",
    "Consider the neoclassical growth model from the lecture. In this question, we extend it so that the production function contains *energy* $m_t$ as a third production factor in addition to capital and labor. Hence, output is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    y_t = f(k_t, h_{y,t}, m_t) = A k_t^\\alpha m_t^\\gamma h_{y,t}^{1-\\alpha-\\gamma}\n",
    "\\end{equation}\n",
    "\n",
    "Energy is itself produced by using a part of the labor supply:\n",
    "\n",
    "\\begin{equation}\n",
    "    m_t = \\rho h_{m,t}\n",
    "\\end{equation}\n",
    "\n",
    "which implies that one unit of labor supply creates $\\rho$ units of energy.\n",
    "\n",
    "Solve the planner problem numerically for $T = 30$. Note that lifetime utility is still given by \n",
    "\n",
    "\\begin{equation}\n",
    "    u(c_t, h_t) = \\frac{c^{1-\\nu}}{1-\\nu} - B \\frac{h_t^{1+\\eta}}{1+\\eta}\n",
    "\\end{equation}\n",
    "\n",
    "with $h_t = h_{y,t} + h_{m,t}$. You can use the parameter values from the lecture, and $\\gamma = 0.05$ and $\\rho = 0.9$. \n",
    "\n",
    "In addition, compute the steady state using a root finding algorithm and verify that the planner's sequences for $k_t$, $h_{y,t}$ and $h_{m,t}$ converge to their steady state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utility\n",
    "beta = 0.9      # discount factor\n",
    "nu = 2       # risk-aversion coefficient for consumption\n",
    "eta = 1         # elasticity parameter for labor supply\n",
    "\n",
    "## production\n",
    "alpha = 0.25\n",
    "gamma = 0.05\n",
    "rho = 0.9\n",
    "delta = 0.1\n",
    "## derived\n",
    "A = (1 - beta * (1 - delta))/(alpha*beta)  # productivity\n",
    "B = (1 - alpha) * A * (A - delta)**nu      # parameter for utility function\n",
    "## initial capital stock\n",
    "k0 = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd(k, m, hy):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * k**alpha * m**gamma * hy**(1 - alpha - gamma)\n",
    "\n",
    "def u(c, h):\n",
    "    \"\"\"\n",
    "    Utility function\n",
    "    \"\"\"\n",
    "    return c**(1 - nu)/(1 - nu) - B * h**(1 + eta)/(1 + eta)\n",
    "\n",
    "def objective(x):\n",
    "    \"\"\"\n",
    "    Objective function: lifetime utility\n",
    "    \"\"\"\n",
    "    kp = np.exp( x[:int(len(x)/3)] )\n",
    "    hy = np.exp( x[int(len(x)/3): 2 * int(len(x)/3)] )\n",
    "    hm = np.exp( x[2 * int(len(x)/3):] )\n",
    "\n",
    "    k = np.insert(kp[:T-1], 0, k0)\n",
    "    \n",
    "    return - ( beta**(np.array(range(T))) @ u( cd(k, rho * hm, hy) + (1 - delta) * k - kp, hy + hm)\n",
    "              + (beta**T/(1 - beta)) *  u( cd(kp[-1], rho * hm[-1], hy[-1]) - delta * kp[-1] , hy[-1] + hm[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cobb_douglas(x, alpha, gamma, A):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * x[0]**alpha * (rho * x[2])**(gamma) * x[1]**(1 - alpha - gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_diff(x, alpha, gamma, A):\n",
    "    \"\"\"\n",
    "    Returns the first derivative of the cobb douglas wrt k, hy and hm\n",
    "    \"\"\"\n",
    "    return (alpha * cobb_douglas(x, alpha, gamma, A) / x[0], \n",
    "            (1 - alpha - gamma) * cobb_douglas(x, alpha, gamma, A) / x[1],\n",
    "            gamma * cobb_douglas(x, alpha, gamma, A) / x[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady(x):\n",
    "    \"\"\"\n",
    "    Returns the vector-valued function consisting of the steady-state conditions \n",
    "    \"\"\"\n",
    "    y = np.zeros(3)\n",
    "    X = np.exp(x)\n",
    "    mp = cd_diff(X, alpha, gamma, A)\n",
    "    \n",
    "    y[0] = beta * (mp[0] + 1 - delta) - 1\n",
    "    y[1] = (cobb_douglas(X, alpha, gamma, A) - delta * X[0])**(-nu) * mp[1] - B * (X[2] + X[1])**eta\n",
    "    y[2] = mp[1] - mp[2]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: array([  0.00000000e+00,  -2.19611203e-06,   1.11022302e-16])\n",
      " message: 'A solution was found at the specified tolerance.'\n",
      "     nit: 87\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([ 0.22552132,  0.40848251, -2.23057482])\n",
      "Steady state: (K_ss, hy_ss, hm_ss) = [ 1.25297575  1.50453294  0.10746664]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.log([1.2, 1.4, 0.2])\n",
    "res_ss = scipy.optimize.root(steady, x0, method = 'broyden1', options = {'line_search' : None, 'jac_options': {'alpha': 1}})\n",
    "print(res_ss)\n",
    "\n",
    "kss = np.exp(res_ss.x)[0]\n",
    "hyss = np.exp(res_ss.x)[1]\n",
    "hmss = np.exp(res_ss.x)[2]\n",
    "\n",
    "print( \"Steady state: (K_ss, hy_ss, hm_ss) = {}\".format(np.exp(res_ss.x) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the model numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "T = 30\n",
    "\n",
    "## set initial guess    \n",
    "x0 = np.concatenate([np.log( 1.2 ) * np.ones(T), np.log( 1.5 ) * np.ones(T), np.log( 0.09 ) * np.ones(T)])\n",
    "\n",
    "## solve model\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', tol = 1e-6)\n",
    "print(res.message)\n",
    "print(res.success)\n",
    "x = res.x\n",
    "kp = np.exp( x[:int(len(x)/3)] )\n",
    "hy = np.exp( x[int(len(x)/3): 2 * int(len(x)/3)] )\n",
    "hm = np.exp( x[2 * int(len(x)/3):] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 15.95642654448408\n",
       " hess_inv: array([[  4.45134976e-01,   2.51824002e-01,   1.53158855e-01, ...,\n",
       "          1.53930886e-02,  -4.54472733e-02,  -4.35948149e-02],\n",
       "       [  2.51824002e-01,   5.60925438e-01,   3.33934811e-01, ...,\n",
       "         -1.71873084e-02,  -8.93501111e-02,  -9.42782943e-02],\n",
       "       [  1.53158855e-01,   3.33934811e-01,   6.26829535e-01, ...,\n",
       "          1.75856982e-01,   1.43429411e-01,  -9.23418143e-02],\n",
       "       ..., \n",
       "       [  1.53930886e-02,  -1.71873084e-02,   1.75856982e-01, ...,\n",
       "          3.95471746e+01,   4.06981974e+01,   1.60457742e-01],\n",
       "       [ -4.54472733e-02,  -8.93501111e-02,   1.43429411e-01, ...,\n",
       "          4.06981974e+01,   1.17026682e+02,  -3.74653360e+00],\n",
       "       [ -4.35948149e-02,  -9.42782943e-02,  -9.23418143e-02, ...,\n",
       "          1.60457742e-01,  -3.74653360e+00,   9.02883232e+00]])\n",
       "      jac: array([  2.38418579e-07,   3.57627869e-07,  -8.34465027e-07,\n",
       "         2.38418579e-07,   1.19209290e-07,  -1.19209290e-07,\n",
       "         1.19209290e-07,  -1.19209290e-07,   2.38418579e-07,\n",
       "         3.57627869e-07,   2.38418579e-07,   2.38418579e-07,\n",
       "         1.19209290e-07,   0.00000000e+00,   1.19209290e-07,\n",
       "         3.57627869e-07,   2.38418579e-07,  -1.19209290e-07,\n",
       "         0.00000000e+00,  -1.19209290e-07,   4.76837158e-07,\n",
       "        -1.19209290e-07,   5.96046448e-07,   1.19209290e-07,\n",
       "         3.57627869e-07,   0.00000000e+00,  -3.57627869e-07,\n",
       "         0.00000000e+00,   2.38418579e-07,   4.76837158e-07,\n",
       "         0.00000000e+00,   2.38418579e-07,   2.38418579e-07,\n",
       "        -1.19209290e-07,   0.00000000e+00,   0.00000000e+00,\n",
       "        -3.57627869e-07,  -3.57627869e-07,   3.57627869e-07,\n",
       "         0.00000000e+00,   1.19209290e-07,  -3.57627869e-07,\n",
       "        -1.19209290e-07,  -2.38418579e-07,   1.19209290e-07,\n",
       "         0.00000000e+00,  -4.76837158e-07,  -4.76837158e-07,\n",
       "        -1.19209290e-07,  -8.34465027e-07,   0.00000000e+00,\n",
       "        -7.15255737e-07,   4.76837158e-07,  -3.57627869e-07,\n",
       "         7.15255737e-07,   2.38418579e-07,  -3.57627869e-07,\n",
       "         1.19209290e-07,  -3.57627869e-07,  -2.38418579e-07,\n",
       "         0.00000000e+00,   0.00000000e+00,  -3.57627869e-07,\n",
       "         2.38418579e-07,  -7.15255737e-07,   4.76837158e-07,\n",
       "        -1.19209290e-07,  -4.76837158e-07,   2.38418579e-07,\n",
       "         2.38418579e-07,  -3.57627869e-07,  -5.96046448e-07,\n",
       "         0.00000000e+00,   4.76837158e-07,   3.57627869e-07,\n",
       "        -2.38418579e-07,  -4.76837158e-07,  -4.76837158e-07,\n",
       "        -1.19209290e-07,   2.38418579e-07,   4.76837158e-07,\n",
       "         1.19209290e-07,  -3.57627869e-07,  -5.96046448e-07,\n",
       "        -2.38418579e-07,   1.19209290e-07,   3.57627869e-07,\n",
       "         2.38418579e-07,  -4.76837158e-07,  -3.57627869e-07])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 12696\n",
       "      nit: 133\n",
       "     njev: 138\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-0.09705559, -0.00977077,  0.05218411,  0.09692882,  0.12964138,\n",
       "        0.15377044,  0.17168207,  0.18504112,  0.19504148,  0.20254588,\n",
       "        0.20818817,  0.21243567,  0.21563799,  0.21805487,  0.21988061,\n",
       "        0.22125882,  0.22229765,  0.22308238,  0.2236778 ,  0.2241271 ,\n",
       "        0.22447083,  0.22472711,  0.22492555,  0.22506944,  0.22518525,\n",
       "        0.22527359,  0.22534304,  0.22540667,  0.22545189,  0.22547629,\n",
       "        0.50575611,  0.48029635,  0.46183344,  0.44829944,  0.43829984,\n",
       "        0.43086652,  0.42531792,  0.42116231,  0.41804322,  0.4156955 ,\n",
       "        0.41392923,  0.41259552,  0.41158956,  0.41082953,  0.41025807,\n",
       "        0.4098252 ,  0.40949551,  0.40924799,  0.40906238,  0.40891554,\n",
       "        0.40881464,  0.40872839,  0.40867935,  0.40862435,  0.40860125,\n",
       "        0.40856828,  0.40854165,  0.40853945,  0.40852888,  0.40848445,\n",
       "       -2.13330179, -2.15876088, -2.17723067, -2.1907528 , -2.20077657,\n",
       "       -2.20817785, -2.21374091, -2.2179096 , -2.2210069 , -2.22335011,\n",
       "       -2.22514534, -2.22649234, -2.22747341, -2.22819761, -2.22877859,\n",
       "       -2.22925009, -2.22961104, -2.22985484, -2.22999922, -2.23009568,\n",
       "       -2.23019639, -2.23031573, -2.23043808, -2.23052484, -2.23053889,\n",
       "       -2.23048251, -2.23041079, -2.23043454, -2.23070591, -2.230583  ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x5ece51438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFJCAYAAAC2OXUDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhGDRJiIiihD/P1ZciPtWIbfCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5ec588a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3,1)\n",
    "ax[0].plot(kp)\n",
    "ax[1].plot(hy)\n",
    "ax[2].plot(hm)\n",
    "ax[0].hlines(kss, 0, T)\n",
    "ax[1].hlines(hyss, 0, T)\n",
    "ax[2].hlines(hmss, 0, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (N)\n",
    "\n",
    "In this question, we are going to apply the gradient descent minimization algorithm on a least-squares regression problem. Consider the Bundesliga data set used in the *Introduction to Python* section of this class. Let's assume we would like to regress a player's market value on his age, his number of goals and assists. Running the following cell (i) reads in the relevant columns of the data set; (ii) creates a matrix **X** with the explanatory variables; and (iii) creates an array **y** containing the dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'value')\n",
      "(4, 'age')\n",
      "(5, 'goals')\n",
      "(6, 'assists')\n",
      "(291, 3)\n",
      "(291, 1)\n",
      "[[ 28.  30.   4.]\n",
      " [ 27.  29.   2.]\n",
      " [ 27.   5.  12.]\n",
      " [ 27.   5.   4.]\n",
      " [ 26.   4.   3.]\n",
      " [ 20.   6.  11.]\n",
      " [ 26.   2.   2.]\n",
      " [ 28.  10.   2.]\n",
      " [ 20.   2.   1.]\n",
      " [ 20.   2.   1.]]\n"
     ]
    }
   ],
   "source": [
    "cols=(2,4,5,6)\n",
    "D = np.loadtxt('BundesligaData.txt', delimiter=';',usecols=(cols), skiprows=1)\n",
    "D[:10, :]\n",
    "\n",
    "description = ['name', 'position', 'value', 'valuemax', 'age', 'goals','assists', 'yellow', 'red', 'shotspergame','passsuccess','aerialswon', 'rating', 'positioncode']\n",
    "for i in cols:\n",
    "    print((i,description[i]))\n",
    "\n",
    "X = D[:,1:]\n",
    "## dependent variable\n",
    "y = D[:,0] \n",
    "y.shape=(D.shape[0], 1)\n",
    "# Before regressing the values, we should check whether X and y have the right shape\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[:10, :])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Scale the values in **X** and add a column of ones. \n",
    "\n",
    "(b) For comparison, use the normal equation to compute $\\mathbf{b}$.\n",
    "\n",
    "(c) Implement the gradient descent algorithm outlined above to find $\\mathbf{b}$. Assume that the step size $\\alpha$ is constant. You may have to play around with $\\alpha$ to find a value that gives you convergence. *Hint*: Recall that in the context of gradient descent, convergence may be slow. When implementing the algorithm above with a **while** loop, you should (as we always do) include a condition that the loop stops after a certain number of iterations, **maxit**. Make sure to set **maxit** sufficiently high in order to get convergence.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use scikit-learn for feature scaling\n",
    "X = preprocessing.scale(X)    \n",
    "## add column of ones to X    \n",
    "X = np.column_stack((np.ones( X.shape[0] ), X ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.12285223]\n",
      " [-1.31745985]\n",
      " [ 3.78177336]\n",
      " [ 1.37891467]]\n"
     ]
    }
   ],
   "source": [
    "## run OLS\n",
    "b = np.linalg.inv((X.T @ X)) @ X.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_linear(b, X, y, alpha, show_it = False, maxit = 50000, eps = 1e-8):\n",
    "    \"\"\"\n",
    "    Implements the iterative procedure for gradient descent in the context of linear regression. Inputs are \n",
    "    -> b: initial guess for the minimizing coefficient vector\n",
    "    -> X: the m-by-n regression matrix, with each row containing the n features for one observation\n",
    "    -> y: the m-by-1 vector of target values \n",
    "    -> alpha: a scalar indicating the step size from b(k) to b(k+1)\n",
    "    \"\"\"\n",
    "    dist = 1\n",
    "    it = 0\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    while dist > eps and it < maxit:\n",
    "        it += 1\n",
    "    \n",
    "        s = -alpha * m**(-1) * X.T @ ( X @ b - y )\n",
    "    \n",
    "        dist = np.linalg.norm(s) / (1 + np.linalg.norm(b))\n",
    "  \n",
    "        b = b + s\n",
    "    \n",
    "    if show_it:\n",
    "        print(\"Gradient descent has converged in {} iterations\".format(it))\n",
    "    \n",
    "    return b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial guess for coefficients\n",
    "b0 = np.array([[0, -0.1, 0.3, 0.1]]).T\n",
    "## set learning rate\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent has converged in 15327 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.12285046],\n",
       "       [-1.31743475],\n",
       "       [ 3.7816535 ],\n",
       "       [ 1.37902944]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd_linear(b0, X, y, alpha, show_it = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
