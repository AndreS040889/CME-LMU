{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Problem Set 3 - Numerical Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2017-12-22 10:06:46.063931\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (N)\n",
    "\n",
    "*From Judd(1998), chapter 4, question 2*. One of the classical uses of optimization is the computation of the *Pareto frontier*. Consider the endowment economy with $m$ goods and $n$ agents. Assume that agent $i$'s utility function over the $m$ goods is \n",
    "\n",
    "\\begin{equation}\n",
    "    u^{i}(x^i) = \\sum^{m}_{j = 1} a^i_j (x^i_j)^{v^i_j + 1} (1 + v^i_j)^{-1} \n",
    "\\end{equation}\n",
    "\n",
    "Suppose that agent $i$'s endowment of good $j$ is $e^i_j$. Assume that $a^i_j, e^i_j > 0 > v^i_j$ (for $v^i_j$, we replace $(x^i_j)^{v^i_j + 1} (1 + v^i_j)^{-1}$ with $\\ln x^i_j$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Write a program using Scipy's BFGS implementation that will read in the $v^i_j$, $a^i_j$ and $e^i_j$ and the social weights $\\lambda^i$, and output the solution to the social planner's problem. Choose $m = n = 2$ and solve the problem *analytically* for $\\lambda_1 = \\lambda_2 = 0.5$ and the following values for the remaining parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V = [[-4 -2]\n",
      " [-3 -3]]\n",
      "A = [[1 4]\n",
      " [1 8]]\n",
      "E = [[6 4]\n",
      " [5 1]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[6, 4], [5, 1]])\n",
    "V = np.array([[-4, -2], [-3, -3]])\n",
    "A = np.array([[1, 4], [1, 8]])\n",
    "\n",
    "print(\"V = {}\".format(V) )\n",
    "print(\"A = {}\".format(A) )\n",
    "print(\"E = {}\".format(E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to read these matrices is that a good corresponds to a row and an agent to a column. For example, agent 1's endowment of good 2, $e^1_2$, would be the element in the second row and first column of matrix **E**, and hence $e^1_2 = 5$.\n",
    "\n",
    "With these parameter values, confirm your analytical result equals the numerical output of your program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Test your program for higher numbers of goods and agents. You can create the parameter matrices above using Numpy's **np.random.uniform** function. Can your program handle $m = n = 5$? $m = n = 10$?      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: A slightly tricky issue when answering this question using *unconstrained* numerical optimization methods is how to deal with the constraint that aggregate consumption of good $j$ must equal aggregate endowments, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum^{n}_{i = 1} x_j^i = \\sum^{n}_{i = 1} e_j^i\n",
    "\\end{equation}\n",
    "\n",
    "One way to address this is to have the algorithm solve for the optimal consumption for $n - 1$ agents and evaluate the consumption and hence the utility of the last agent *as the residual*. formally, for good $j$,\n",
    "\n",
    "\\begin{equation}\n",
    "    x_j^n = \\sum^{n}_{i = 1} e_j^i - \\sum^{n-1}_{i = 1} x_j^i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "For the analytical solution to the 2-by-2 case, compare the notes in class. \n",
    "\n",
    "For the numerical implementation, start by addressing the issue of the endowment constraints discussed above. The function **consum(x, E)** takes a vector **x** (the consumption levels for the first $n-1$ agents) and the endowment matrix **E** and returns a consumption matrix **X**, in which the last column (the consumption of agent $n$) is computed as the residual.\n",
    "\n",
    "As a side note, why do we define **consum** in a way that it takes a vector (i.e. a one-dimensional array) rather than a two-dimensional matrix as an input? Note that **scipy.optimize.minimize** returns a vector (even if we use a matrix as an initial, as seen below!) and hence we need to write **consum** in a way that it accommodates this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consum(x, E):\n",
    "    \"\"\"\n",
    "    Computes an m-by-n consumption matrix from solution vector x and endowments E;\n",
    "    Agent n's consumption is computed as the residual comsumption \n",
    "    \"\"\"\n",
    "    ## get dimension of the endowment matrix\n",
    "    m, n = E.shape \n",
    "    ## check that x and E have consistent dimensions\n",
    "    assert len(x) == m * (n-1), \"The length is not consistent with the dimensions of E!\"\n",
    "    \n",
    "    ## reshape x into matrix X \n",
    "    X = x.copy()\n",
    "    X.shape = m, n - 1\n",
    "    \n",
    "    ## compute residual consumption\n",
    "    x_res = E.sum(axis = 1) - X.sum(axis = 1)\n",
    "    \n",
    "    ## combine X with x_res and return\n",
    "    return np.column_stack( (X, x_res) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **consum**, it is straightforward to implement the objective function $U = \\sum^n_{i = 1} \\lambda^i u^{i}(x^i)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, V, A, E, lam):\n",
    "    \"\"\"\n",
    "    Computes the objective function for the m-by-n social planner problem\n",
    "    \"\"\"\n",
    "    ## get dimension of the problem \n",
    "    m, n = V.shape ## number of agents, number of good\n",
    "\n",
    "    ## check if the parameter matrices have the correct dimensions \n",
    "    assert A.shape == (m, n), \"The dimensions of A and V must coincide!\"\n",
    "    assert E.shape == (m, n), \"The dimensions of E and V must coincide!\"\n",
    "    assert len(lam) == n, \"The length of lam is not consistent with the dimensions of V!\"\n",
    "    \n",
    "    ## compute matrices of consumption using x and endowment matrix E\n",
    "    X = consum(x, E)\n",
    "    \n",
    "    ## loop over agents and evaluate their utility\n",
    "    u = np.zeros(n)\n",
    "    for ind in range(n):\n",
    "        v, a = V[:, ind], A[:, ind]\n",
    "        Q = a / (1 + v)\n",
    "        u[ind] = Q @ X[:,ind]**(1 + v)\n",
    "\n",
    "    ## compute welfare given weights lam (NB: we could use summation instead of vector multiplication )\n",
    "    return - lam @ u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the problem using the BFGS algorithm for 2-by-2 case. As an initial guess, use the agents' endowment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.458333\n",
      "         Iterations: 16\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 18\n",
      "Consumption matrix: \n",
      "[[ 1.99999254  8.00000746]\n",
      " [ 2.00000424  3.99999576]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[6, 4], [5, 1]])\n",
    "V = np.array([[-4, -2], [-3, -3]])\n",
    "A = np.array([[1, 4], [1, 8]])\n",
    "\n",
    "x0 =  E[:, :-1]\n",
    "lam = np.array([0.5, 0.5])\n",
    "objective(x0, V, A, E, lam)\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m = 3, n = 2\n",
    "\n",
    "We can check that the algorithm works in a setting with more goods than agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.329515\n",
      "         Iterations: 8\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 9\n",
      "Consumption matrix: \n",
      "[[ 1.69723441  1.30276559]\n",
      " [ 1.47265462  2.52734538]\n",
      " [ 1.34772323  3.65227677]]\n"
     ]
    }
   ],
   "source": [
    "E = np.array([[2, 1], [2, 2], [2, 3]])\n",
    "V = np.array([[-2, -4], [-3, -2], [-5, -2]])\n",
    "A = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "\n",
    "x0 =  E[:, :-1]\n",
    "lam = np.array([0.5, 0.5])\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m = 5, n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 5, 5\n",
    "E = np.random.uniform(0.01, 5, (m, n))\n",
    "V = np.random.uniform(-5, -1.001, (m, n))\n",
    "A = np.random.uniform(1, 3, (m, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.698716\n",
      "         Iterations: 107\n",
      "         Function evaluations: 2420\n",
      "         Gradient evaluations: 110\n",
      "Consumption matrix: \n",
      "[[ 3.70270958  3.33976585  3.88274306  3.14770283  3.54214685]\n",
      " [ 2.34565267  2.76808966  4.37935899  1.69298758  3.13704377]\n",
      " [ 1.78522301  2.0089305   1.56764613  2.7177349   2.53509534]\n",
      " [ 1.81892515  7.07260702  3.39646574  2.24920332  1.52595427]\n",
      " [ 3.06973191  2.78861986  4.89620603  3.28187768  4.70086454]]\n"
     ]
    }
   ],
   "source": [
    "x0 = E[:, :-1]\n",
    "lam = np.ones(n) * (1/n)\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', args = (V, A, E, lam), options = {'disp': True})\n",
    "x = res.x\n",
    "\n",
    "print(\"Consumption matrix: \")\n",
    "print( consum(x, E) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (N)\n",
    "\n",
    "Consider the neoclassical growth model from the lecture. In this question, we extend it so that the production function contains *energy* $m_t$ as a third production factor in addition to capital and labor. Hence, output is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    y_t = f(k_t, h_{y,t}, m_t) = A k_t^\\alpha m_t^\\gamma h_{y,t}^{1-\\alpha-\\gamma}\n",
    "\\end{equation}\n",
    "\n",
    "Energy is itself produced by using a part of the labor supply:\n",
    "\n",
    "\\begin{equation}\n",
    "    m_t = \\rho h_{m,t}\n",
    "\\end{equation}\n",
    "\n",
    "which implies that one unit of labor supply creates $\\rho$ units of energy.\n",
    "\n",
    "Solve the planner problem numerically for $T = 30$. Note that lifetime utility is still given by \n",
    "\n",
    "\\begin{equation}\n",
    "    u(c_t, h_t) = \\frac{c^{1-\\nu}}{1-\\nu} - B \\frac{h_t^{1+\\eta}}{1+\\eta}\n",
    "\\end{equation}\n",
    "\n",
    "with $h_t = h_{y,t} + h_{m,t}$. You can use the parameter values from the lecture, and $\\gamma = 0.05$ and $\\rho = 0.9$. \n",
    "\n",
    "In addition, compute the steady state using a root finding algorithm and verify that the planner's sequences for $k_t$, $h_{y,t}$ and $h_{m,t}$ converge to their steady state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utility\n",
    "beta = 0.9      # discount factor\n",
    "nu = 2       # risk-aversion coefficient for consumption\n",
    "eta = 1         # elasticity parameter for labor supply\n",
    "\n",
    "## production\n",
    "alpha = 0.25\n",
    "gamma = 0.05\n",
    "rho = 0.9\n",
    "delta = 0.1\n",
    "## derived\n",
    "A = (1 - beta * (1 - delta))/(alpha*beta) # normalization parameter for production function => steady state k = 1\n",
    "B = (1 - alpha) * A * (A - delta)**nu      # parameter for utility function\n",
    "## initial capital stock\n",
    "k0 = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd(k, m, hy):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * k**alpha * m**gamma * hy**(1 - alpha - gamma)\n",
    "\n",
    "def u(c, h):\n",
    "    \"\"\"\n",
    "    Utility function\n",
    "    \"\"\"\n",
    "    return c**(1 - nu)/(1 - nu) - B * h**(1 + eta)/(1 + eta)\n",
    "\n",
    "def objective(x):\n",
    "    \"\"\"\n",
    "    Objective function: lifetime utility\n",
    "    \"\"\"\n",
    "    kp = np.exp( x[:int(len(x)/3)] )\n",
    "    hy = np.exp( x[int(len(x)/3): 2 * int(len(x)/3)] )\n",
    "    hm = np.exp( x[2 * int(len(x)/3):] )\n",
    "\n",
    "    k = np.insert(kp[:T-1], 0, k0)\n",
    "    \n",
    "    return - ( beta**(np.array(range(T))) @ u( cd(k, rho * hm, hy) + (1 - delta) * k - kp, hy + hm)\n",
    "              + (beta**T/(1 - beta)) *  u( cd(kp[-1], rho * hm[-1], hy[-1]) - delta * kp[-1] , hy[-1] + hm[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cobb_douglas(x, alpha, gamma, A):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * x[0]**alpha * (rho * x[2])**(gamma) * x[1]**(1 - alpha - gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_diff(x, alpha, gamma, A):\n",
    "    \"\"\"\n",
    "    Returns the first derivative of the cobb douglas wrt k, hy and hm\n",
    "    \"\"\"\n",
    "    return (alpha * cobb_douglas(x, alpha, gamma, A) / x[0], \n",
    "            (1 - alpha - gamma) * cobb_douglas(x, alpha, gamma, A) / x[1],\n",
    "            gamma * cobb_douglas(x, alpha, gamma, A) / x[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady(x):\n",
    "    \"\"\"\n",
    "    Returns the vector-valued function consisting of the steady-state conditions \n",
    "    \"\"\"\n",
    "    y = np.zeros(3)\n",
    "    X = np.exp(x)\n",
    "    mp = cd_diff(X, alpha, gamma, A)\n",
    "    \n",
    "    y[0] = beta * (mp[0] + 1 - delta) - 1\n",
    "    y[1] = (cobb_douglas(X, alpha, gamma, A) - delta * X[0])**(-nu) * mp[1] - B * (X[2] + X[1])**eta\n",
    "    y[2] = mp[1] - mp[2]\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: array([  9.70518538e-07,  -7.92234419e-07,  -3.29659095e-07])\n",
      " message: 'A solution was found at the specified tolerance.'\n",
      "     nit: 14\n",
      "  status: 1\n",
      " success: True\n",
      "       x: array([ 0.22551378,  0.40848182, -2.23057618])\n",
      "Steady state: (K_ss, hy_ss, hm_ss) = [ 1.2529663   1.50453191  0.10746649]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schmitt\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\nonlin.py:997: RuntimeWarning: invalid value encountered in true_divide\n",
      "  d = v / vdot(df, v)\n"
     ]
    }
   ],
   "source": [
    "x0 = np.log([1.2, 1.4, 0.2])\n",
    "res_ss = scipy.optimize.root(steady, x0, method = 'broyden1')\n",
    "print(res_ss)\n",
    "\n",
    "kss = np.exp(res_ss.x)[0]\n",
    "hyss = np.exp(res_ss.x)[1]\n",
    "hmss = np.exp(res_ss.x)[2]\n",
    "\n",
    "print( \"Steady state: (K_ss, hy_ss, hm_ss) = {}\".format(np.exp(res_ss.x) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the model numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "T = 30\n",
    "\n",
    "## set initial guess    \n",
    "x0 = np.concatenate([np.log( 1.2 ) * np.ones(T), np.log( 1.5 ) * np.ones(T), np.log( 0.09 ) * np.ones(T)])\n",
    "\n",
    "## solve model\n",
    "res = scipy.optimize.minimize(objective, x0, method = 'BFGS', tol = 1e-6)\n",
    "print(res.message)\n",
    "print(res.success)\n",
    "x = res.x\n",
    "kp = np.exp( x[:int(len(x)/3)] )\n",
    "hy = np.exp( x[int(len(x)/3): 2 * int(len(x)/3)] )\n",
    "hm = np.exp( x[2 * int(len(x)/3):] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x1d801f1b00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFJCAYAAAC2OXUDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgoTLNpERERh4v8HFp2R2nXDi7AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8006c978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3,1)\n",
    "ax[0].plot(kp)\n",
    "ax[1].plot(hy)\n",
    "ax[2].plot(hm)\n",
    "ax[0].hlines(kss, 0, T)\n",
    "ax[1].hlines(hyss, 0, T)\n",
    "ax[2].hlines(hmss, 0, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (N)\n",
    "\n",
    "In this question, we are going to apply the gradient descent minimization algorithm on a least-squares regression problem. Recall that when using least squares, we minimize the sum of squared residuals. For example, for $m$ observations and four regressors (including a constant), we solve\n",
    "\n",
    "\\begin{equation}\n",
    "    \\min_{b_0, b_1, b_2, b_3} L = \\sum_{i = 1}^m \\left[y_i - ( b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i3} )\\right]^2\n",
    "\\end{equation}\n",
    "\n",
    "with the predicted values given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y}_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i3} . \n",
    "\\end{equation}\n",
    "\n",
    "We refer to $L$ as the *loss function*. Its gradient is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L}{\\partial b_j} = -2 \\sum_{i = 1}^m \\left[y_i - \\hat{y}_i\\right] x_{ij} = 2\\mathbf{X}_j' \\cdot (\\hat{\\mathbf{y} } - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n",
    "for $j=0,1,2,3$. $\\mathbf{X}_j$ denotes the $j$th column in the regression matrix. Hence,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla L = \\left[ \n",
    "        \\begin{array}{c}\n",
    "        \\frac{\\partial L}{\\partial b_0} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial L}{\\partial b_3}\n",
    "        \\end{array}\n",
    "    \\right] =  2 \\mathbf{X}' \\cdot(\\hat{\\mathbf{y} } - \\mathbf{y}) \n",
    "\\end{equation}\n",
    "\n",
    "with the predicted values given by $\\hat{\\mathbf{y}} = \\mathbf{X} \\mathbf{b}$.\n",
    "\n",
    "Recall that gradient descent uses the familiar update rule\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{b}^{(k+1)} = \\mathbf{b}^{(k)} + \\alpha^{(k)} \\mathbf{p}^{(k)} = \\mathbf{b}^{(k)} - \\alpha^{(k)} \\nabla  L(\\mathbf{b}^{(k)} )\n",
    "\\end{equation}\n",
    "\n",
    "In other words, the search direction used by gradient descent is the gradient of the objective function (here the loss function) at the current guess for $\\mathbf{b}^{(k)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an application, consider the Bundesliga data set used in the *Introduction to Python* section of this class. Let's assume we would like to regress a player's market value on his age, his number of goals and assists. Running the following cell (i) reads in the relevant columns of the data set; (ii) creates a matrix **X** with the explanatory variables (in logs) and a constant; and (iii) creates an array **y** containing the dependent variables (in logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'value')\n",
      "(4, 'age')\n",
      "(5, 'goals')\n",
      "(6, 'assists')\n",
      "(291, 4)\n",
      "(291, 1)\n",
      "[[ 1.          3.36729583  3.4339872   1.60943791]\n",
      " [ 1.          3.33220451  3.40119738  1.09861229]\n",
      " [ 1.          3.33220451  1.79175947  2.56494936]\n",
      " [ 1.          3.33220451  1.79175947  1.60943791]\n",
      " [ 1.          3.29583687  1.60943791  1.38629436]\n",
      " [ 1.          3.04452244  1.94591015  2.48490665]\n",
      " [ 1.          3.29583687  1.09861229  1.09861229]\n",
      " [ 1.          3.36729583  2.39789527  1.09861229]\n",
      " [ 1.          3.04452244  1.09861229  0.69314718]\n",
      " [ 1.          3.04452244  1.09861229  0.69314718]]\n"
     ]
    }
   ],
   "source": [
    "cols=(2,4,5,6)\n",
    "D = np.loadtxt('BundesligaData.txt', delimiter=';',usecols=(cols), skiprows=1)\n",
    "D[:10, :]\n",
    "\n",
    "description = ['name', 'position', 'value', 'valuemax', 'age', 'goals','assists', 'yellow', 'red', 'shotspergame','passsuccess','aerialswon', 'rating', 'positioncode']\n",
    "for i in cols:\n",
    "    print((i,description[i]))\n",
    "    \n",
    "X = np.column_stack((np.ones( D.shape[0] ), np.log( D[:,1:] + 1 )))\n",
    "## dependent variable\n",
    "y = np.log( D[:,0] )\n",
    "y.shape=(D.shape[0], 1)\n",
    "# Before regressing the values, we should check whether X and y have the right shape\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X[:10, :])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(As a side note: in practice, it would be much more convenient to work with the dataset as a *Pandas dataframe*. Since using Pandas is not required for this course, we use a standard Numpy array instead)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) For comparison, use the standard formula for the OLS estimator to compute $\\mathbf{b}$.\n",
    "\n",
    "(b) Implement the gradient descent algorithm outlined above to find $\\mathbf{b}$. Assume that the step size $\\alpha$ is constant. You may have to play around with $\\alpha$ to find a value that gives you convergence. *Hint*: Recall that in the context of gradient descent, convergence may be slow. When implementing the algorithm above with a **while** loop, you should (as we always do) include a condition that the loop stops after a certain number of iterations, **maxit**. Make sure to set **maxit** sufficiently high in order to get convergence.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.69323692]\n",
      " [-1.35906677]\n",
      " [ 0.16772997]\n",
      " [ 0.31647362]]\n"
     ]
    }
   ],
   "source": [
    "## run OLS\n",
    "b = np.linalg.inv((X.T @ X)) @ X.T @ y\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.69327902]\n",
      " [-1.35907967]\n",
      " [ 0.16773014]\n",
      " [ 0.31647362]] 9.99972209669e-09 52532\n"
     ]
    }
   ],
   "source": [
    "## initial guess for coefficients\n",
    "b0 = np.array([[13, -0.3, 0.9, 0.6]]).T\n",
    "## set learning rate\n",
    "alpha = 5e-4\n",
    "## parameters for loop\n",
    "it, it_max = 0, 60000\n",
    "dist = 1\n",
    "tol = 1e-8\n",
    "\n",
    "## initialize coeffcient vector as column vector\n",
    "# a = 0.8 * beta\n",
    "b = b0\n",
    "while dist > tol and it <= it_max:\n",
    "    it += 1\n",
    "\n",
    "    ## use gradient descent to get new guess \n",
    "    b_new = b - (alpha) *  X.T @ ( X @ b - y )\n",
    "    \n",
    "    ## find difference between old and new coefficients\n",
    "    dist = np.linalg.norm(b - b_new)\n",
    "    ## update current guess\n",
    "    b = b_new    \n",
    "\n",
    "print(b_new, dist, it)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
