{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Methods in Economics\n",
    "\n",
    "## Lecture 4 - Root Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last update: 2017-11-03 17:42:01.221204\n"
     ]
    }
   ],
   "source": [
    "# Author: Alex Schmitt (schmitt@ifo.de)\n",
    "\n",
    "import datetime\n",
    "print('Last update: ' + str(datetime.datetime.today()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "import scipy.optimize\n",
    "\n",
    "# import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A function $f(x)$ has a *root* (also called a *zero*) at $x^*$ if $f(x^*) = 0$. Note that $f$ here can be a univariate function (both input and output are scalars, or both its range and its domain have a dimension of 1), a multivariate function (its inputs are vectors, hence its domain has a dimension greater than 1) or vector-valued (both its range and its domain have a dimension greater than 1. In the latter case, finding the roots of a vector-valued function is equivalent to *solving a system of nonlinear equations*. \n",
    "\n",
    "Finding the root(s) of a function is one of the most common computational problems in economics, often applied when looking for an equilibrium. In other words, an equilibrium is usually defined by a set of equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Neoclassical Growth Model\n",
    "\n",
    "\n",
    "- Utility function:\n",
    "\n",
    "\\begin{equation}\n",
    "    u(c, h) = \\frac{c^{1-\\nu}}{1-\\nu} - B \\frac{h^{1+\\eta}}{1+\\eta}\n",
    "\\end{equation}\n",
    "\n",
    "with $c$ denoting consumption and $h$ labor supply.\n",
    "\n",
    "- Production function:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(k, h) = A k^\\alpha h^{1-\\alpha}\n",
    "\\end{equation}\n",
    "with $k$ denoting the capital stock, and $\\theta$ a productivity shock.\n",
    "\n",
    "- Resource Constraint:\n",
    "\n",
    "\\begin{equation}\n",
    "    k_{t+1} + c_t = f(k_t, h_t) + (1 - \\delta) k_t = A k_t^\\alpha h_t^{1-\\alpha} + (1 - \\delta) k_t\n",
    "\\end{equation}\n",
    "\n",
    "- Planner's Problem:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\max_{\\left\\{c_t, k_{t+1}, h_t\\right\\}} \\sum^\\infty_{t = 0} \\beta^t u(c_t, h_t) \n",
    "\\end{equation}\n",
    "s.t. the resource constraint.\n",
    "\n",
    "- F.o.c's:\n",
    "(1) Euler equation\n",
    "\n",
    "\\begin{equation}\n",
    "    c^{-\\nu} = \\beta \\left[ (c')^{-\\nu} (f_k(k', h') + 1 - \\delta) \\right]    \n",
    "\\end{equation}\n",
    "\n",
    "(2) intratemporal optimality condition\n",
    "\n",
    "\\begin{equation}\n",
    "    B h^{\\eta} = c^{-\\nu} f_h(k, h)  \n",
    "\\end{equation}\n",
    "\n",
    "where I have used the notation $c = c_t$ and $c' = c_{t+1}$ for brevity. \n",
    "\n",
    "In an equilibrium, the two first-order conditions, combined with the resource constraint, must hold in every period. We will get to how to solve for the full dynamic allocation later in this course. For now, let's consider the *steady state*, where all variables are constant over time, i.e. $c_t = c_{t+1} = c_s$ and so on. The Euler equation then can be simplied to:\n",
    "\n",
    "\\begin{equation}\n",
    "    1 = \\beta \\left[f_k(k_s, h_s) + 1 - \\delta \\right]    \n",
    "\\end{equation}\n",
    "\n",
    "For the intratemporal optimality condition, use the resource constraint to substitute consumption:\n",
    "\n",
    "\\begin{equation}\n",
    "    B h_s^{\\eta} = \\left[ f(k_s, h_s) - \\delta k_s \\right]^{-\\nu} f_h(k_s, h_s)  \n",
    "\\end{equation}\n",
    "\n",
    "This is a nonlinear system of two equations, with two unknown variables, $k_s$ and $h_s$, which can be solved using the method introduced below. We can also define a vector-valued function $\\mathbf{S}$ with\n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}(k, h) = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        \\beta \\left[f_k(k, h) + 1 - \\delta \\right]  - 1 \\\\\n",
    "        \\left[ f(k, h) - \\delta k \\right]^{-\\nu} f_h(k, h) - B h^{\\eta}\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Finding the steady state of the model then requires finding a root of function $\\mathbf{S}$, i.e. a vector $(k_s, h_s)$ such that \n",
    "\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}(k_s, h_s) = \n",
    "    \\left[\n",
    "    \\begin{array}{c}\n",
    "        0 \\\\\n",
    "        0\n",
    "    \\end{array}\n",
    "    \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to compute the root of a univariate real-valued function is the *bisection method*. While simple, bisection captures two important features of most root-finding and optimization methods: it is a *local* method and it is based on an *iterative procedure*.\n",
    "\n",
    "The key idea behind the bisection method is based on the *Intermediate Value Theorem*: if $f$ is continuous and defined on the interval $[a,b]$, and if $f(a)$ and $f(b)$ are distinct values, then $f$ must assume all values in between. Since we are interested in where $f$ assumes the value 0, we need $f(a)$ and $f(b)$ to have different signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bisection method implements the following \"pseudo-code\":\n",
    "\n",
    "(i) Start with two values $a$ and $b$ such that $f(a)$ and $f(b)$ are defined and have different signs. Moreover, specify a \"tolerance level\" $tol$ which should be a very small number, e.g. 1e-8.\n",
    "\n",
    "(ii) Compute the midpoint between $a$ and $b$, $x = \\frac{a + b}{2}$. \n",
    "\n",
    "(iii) If $f(x)$ has the same sign as $f(a)$, replace the left endpoint of the interval with $x$, i.e. $a = x$.\n",
    "\n",
    "(iv) If $f(x)$ has the same sign as $f(b)$, replace the right endpoint of the interval with $x$, i.e. $b = x$.\n",
    "\n",
    "(v) Repeat from (ii) until the absolute value of $f(x)$ is less than $tol$, i.e. $|f(x)| < tol$.\n",
    "\n",
    "Note the following:\n",
    "- Bisection is an *iterative procedure*: at the beginning of each iteration step, the interval $[a,b]$ contains a root of $f$. The interval is then divided (\"bisected\") into two subintervals of equal length. One of the two subintervals must contain the root, and hence have endpoints of different signs. This subinterval is taken as the interval $[a,b]$ used for the next iteration. This process continues until the resulting midpoint $x$ of the current interval is sufficiently close to 0.  \n",
    "- Moreover, bisection is a *local* method: it will not give you all the roots of a function, but only one of the roots (in case there are multiple roots) between $a$ and $b$. A corollary of this is that the outcome of bisection (and of local methods in general) is sensitive to the starting point chosen by the user, here the values for $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's problem set, you will be asked to code up the bisection method. Of course, most programming languages already have in-built implementations (e.g. in SciPy: **scipy.optimize.bisect**, as discussed below), so writing your own function may seem a bit redundant, but will help you to get used the inner workings of many of the algorithms used in scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Iteration\n",
    "\n",
    "We have started to talk about iterative methods at the end of last lecture. To recap, the basic idea of iterative methods is to generate a sequence of approximations to the object of interest, e.g. the solution to linear or nonlinear system of equations, following an iteration rule: \n",
    "\n",
    "\\begin{equation}\n",
    "    x^{(k+1)} = g( x^{(k)} ),\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is an indicator counting the number of iterations. Hence, in words, the value for $x$ in the $k+1$-iteration is obtained by applying function $g$ on the value for $x$ in the $k$-iteration. Ideally, these approximations become more and more precise with an increasing number of iterations. Recall that iterative methods, in contrast to direct methods, do not yield an exact solution.\n",
    "\n",
    "\n",
    "When finding the root of a function $f$ or solving for a system of nonlinear equations, the functional form of $g$ is simply\n",
    "\n",
    "\\begin{equation}\n",
    "    g( x ) = x - f(x).\n",
    "\\end{equation}\n",
    "\n",
    "This is intuitive: at the root $ x = x* $, we have $f(x^*) = 0$ and hence $g (x^*) = x^*$. In other words, $x^*$ is a *fixed point*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code implements function iteration. For illustration, we print the current guess for $x^{(k)}$ for each iteration. As we can see,  $x^{(k)}$ converges to $x^*$ as the number of iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return 4*np.log(x) - 4\n",
    "\n",
    "def g(x):\n",
    "    return x - fun(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45482255552\n",
      "2.86260463623\n",
      "2.65567694682\n",
      "2.74887857583\n",
      "2.70410642422\n",
      "2.72502036232\n",
      "2.71511676033\n",
      "2.7197769279\n",
      "2.71757746733\n",
      "2.71861408156\n",
      "2.7181251951\n",
      "2.71835569051\n",
      "2.71824700267\n",
      "2.71829824977\n",
      "2.71827408559\n",
      "2.71828547937\n",
      "2.71828010699\n",
      "2.71828264016\n",
      "2.71828144573\n",
      "2.71828200892\n",
      "2.71828174337\n",
      "2.71828186858\n",
      "2.71828180954\n",
      "2.71828183738\n",
      "2.71828182425\n",
      "Number of iterations = 25\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-8\n",
    "x = 4\n",
    "it = 0\n",
    "lst = []\n",
    "while abs((x - g(x))) > eps:\n",
    "    it += 1\n",
    "    x = g(x)\n",
    "    lst.append(x)\n",
    "    print(x)\n",
    "    \n",
    "\n",
    "print(\"Number of iterations = {}\".format(it) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "Most algorithms used in practice to find the roots of a nonlinear system of equations are based on Newton's method. As function iteration, it is an iterative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fd(x):\n",
    "    return 4/x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scipy Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, consider the function \n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = 4 \\ln(x) - 4\n",
    "\\end{equation}\n",
    "\n",
    "$f$ has a root at $x = e^x = 2.718282$. To find it numerically, the first thing we need to do is to import Scipy's subpackage *optimize*. We then define the function and use the **bisect()** function, an implementation of the bisection method outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return 4*np.log(x) - 4\n",
    "\n",
    "print(scipy.optimize.bisect(fun,1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bisect(fun,a,b)** takes three arguments: the function, and an upper and lower initial guess for the root. In other words, you tell the algorithm to look for a root in the interval $[a,b]$. The important thing to note here is that $f(a)$ and $f(b)$ must have different signs - if they do not, you will get an error message (in this case, change $a$ or $b$ and try again).  \n",
    "\n",
    "In the example above, solving for the root using Python is not really necessary. The real advantage of numerical root finding is in situations where finding a solution to $f(x) = 0$ analytically is not feasible. Consider, for example,\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1\n",
    "\\end{equation}\n",
    "\n",
    "Finding a root via the bisection method is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4082935042806639\n"
     ]
    }
   ],
   "source": [
    "def fun(x):\n",
    "    return np.sin(4 * (x - 0.25)) + x + x**20 - 1\n",
    "\n",
    "print(scipy.optimize.bisect(fun,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 58.7 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit scipy.optimize.bisect(f,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.14 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 57.7 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit scipy.optimize.fsolve(f,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 53.9 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit scipy.optimize.fsolve(f,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.42 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 59.5 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit scipy.optimize.root(f,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: NGM Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## utility\n",
    "beta = 0.8      # discount factor\n",
    "nu = 2       # risk-aversion coefficient for consumption\n",
    "eta = 1         # elasticity parameter for labor supply\n",
    "eps = 1e-6      # lower bound of consumption and labor supply\n",
    "## production\n",
    "alpha = 0.25\n",
    "delta = 0\n",
    "## derived\n",
    "A = (1 - beta * (1 - delta))/(alpha*beta) # normalization parameter for production function => steady state k = 1\n",
    "B = (1 - alpha) * A * (A - delta)**nu      # parameter for utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cobb_douglas(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return A * x[0]**alpha * x[1]**(1 - alpha)\n",
    "\n",
    "def cd_diff(x, alpha, A):\n",
    "    \"\"\"\n",
    "    Evaluates the first derivative Cobb-Douglas function with coefficient alpha and shift parameter A, for two inputs (x)\n",
    "    \"\"\"\n",
    "    return (A * alpha * cobb_douglas(x, alpha, A) / x[0], A * (1 - alpha) * cobb_douglas(x, alpha, A) / x[1])\n",
    "\n",
    "def steady(x):\n",
    "    \"\"\"\n",
    "    Returns the residuals of the steady-state conditions \n",
    "    \"\"\"\n",
    "    y = np.zeros(2)\n",
    "    mp = cd_diff(x, alpha, A)\n",
    "    \n",
    "    y[0] = beta * (mp[0] + 1 - delta) - 1\n",
    "    y[1] = (cobb_douglas(x, alpha, A) - delta * x[0])**(-nu) * mp[1] - B * x[1]**eta\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve for the steady state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    fjac: array([[-0.22330597, -0.9747484 ],\n",
       "       [ 0.9747484 , -0.22330597]])\n",
       "     fun: array([  1.16562315e-11,   8.72503181e-11])\n",
       " message: 'The solution converged.'\n",
       "    nfev: 11\n",
       "     qtf: array([ -9.08976505e-09,  -2.05845083e-09])\n",
       "       r: array([ 1.34344783,  0.8497232 ,  0.5024392 ])\n",
       "  status: 1\n",
       " success: True\n",
       "       x: array([ 1.,  1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([0.5, 0.5])\n",
    "scipy.optimize.root(steady, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
